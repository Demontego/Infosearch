{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zlib\n",
    "import zipfile\n",
    "import gzip\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(a, b):\n",
    "    time_1=time.time()\n",
    "    \"Calculates the Levenshtein distance between a and b.\"\n",
    "    n, m = len(a), len(b)\n",
    "    if n > m:\n",
    "        # Make sure n <= m, to use O(min(n, m)) space\n",
    "        a, b = b, a\n",
    "        n, m = m, n\n",
    "\n",
    "    current_row = range(n + 1)  # Keep current and previous row, not entire matrix\n",
    "    for i in range(1, m + 1):\n",
    "        previous_row, current_row = current_row, [i] + [0] * n\n",
    "        for j in range(1, n + 1):\n",
    "            add, delete, change = previous_row[j] + 1, current_row[j - 1] + 1, previous_row[j - 1]\n",
    "            if a[j - 1] != b[i - 1]:\n",
    "                change += 1\n",
    "            current_row[j] = min(add, delete, change)\n",
    "    print(time.time()-time_1)\n",
    "    return current_row[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words={}\n",
    "words_l=[]\n",
    "with open('lenta_words.txt', encoding='utf-8') as input_file: \n",
    "    for i in (input_file):\n",
    "        i=i.strip()\n",
    "        if i in words:\n",
    "            value = words[i]\n",
    "            words[i]=value+1\n",
    "        else:\n",
    "            words_l.append(i)\n",
    "            words[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances(word, voc):\n",
    "    dist=[]\n",
    "    for w in voc:\n",
    "        dist.append(distance(word,w))\n",
    "    dist=np.array(dist)\n",
    "    ind = np.where(dist == dist.min())[0]\n",
    "    freq=int(0)\n",
    "    for i in ind:\n",
    "        if freq<words[words_l[i]]:\n",
    "            freq=words[words_l[i]]\n",
    "            word=words_l[i]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=[\"путн\", \"оцинил\",\"роботу\",\"новвых\", \"самалетав\", \"и\",\"виртолтов\", \"сирийи\"]\n",
    "correct_words=[]\n",
    "for i in z:\n",
    "    correct_words.append(distances(i,words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "0\n",
      "0.0\n",
      "0\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "0\n",
      "0.0\n",
      "0\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(z)):\n",
    "    print(distance(z[i],correct_words[i]))\n",
    "    print(distance_2(z[i], correct_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_2(a, b):\n",
    "    # Calculates the Levenshtein distance between a and b\n",
    "    time_1=time.time()\n",
    "    n, m = len(a), len(b)\n",
    "    inverse = False\n",
    "    if n > m:\n",
    "        # Make sure n <= m, to use O(min(n,m)) space\n",
    "        a, b = b, a\n",
    "        n, m = m, n\n",
    "        inverse = True\n",
    "\n",
    "    max_distance = m + n  # to avoid index out of range in possible_action creation, fictive column\n",
    "    current_row = list(range(n + 1)) + [max_distance]  # Keep current and previous row, not entire matrix\n",
    "\n",
    "    matrix = np.array([current_row])\n",
    "    for i in range(1, m + 1):\n",
    "        previous_row, current_row = current_row, [i] + [0] * n + [max_distance]\n",
    "        for j in range(1, n + 1):\n",
    "            # add, delete, change = previous_row[j] + 1, current_row[j - 1] + 1, previous_row[j - 1]\n",
    "            add, delete, change = previous_row[j] + 1, \\\n",
    "                                  current_row[j - 1] + 1, \\\n",
    "                                  previous_row[j - 1] + int(a[j - 1] != b[i - 1])\n",
    "            # if a[j - 1] != b[i - 1]:\n",
    "            #     change += 1\n",
    "            current_row[j] = min(add, delete, change)\n",
    "\n",
    "        matrix = np.vstack((matrix, [current_row]))\n",
    "\n",
    "    lev_distance = current_row[n]\n",
    "    # print(matrix)\n",
    "\n",
    "    if inverse:\n",
    "        matrix = matrix.T\n",
    "        a, b = b, a\n",
    "        n, m = m, n\n",
    "    print(time.time()-time_1)\n",
    "    return lev_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def read(file_name):\n",
    "    with open(file_name, 'r',  encoding='utf-8') as f:\n",
    "        content = f.readlines()\n",
    "        content = [l.strip('\\n') for l in content]\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def flatten_list(lol):\n",
    "    return [item for sublist in lol for item in sublist]\n",
    "\n",
    "\n",
    "def flatten_dictionary(dictionary):\n",
    "    values = []\n",
    "    for d in dictionary.values():\n",
    "        for v in d.values():\n",
    "            values.append(v)\n",
    "    return values\n",
    "\n",
    "\n",
    "def dump(obj, file_name):\n",
    "    with open(file_name, 'w',  encoding='utf-8') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load(file_name):\n",
    "    with open(file_name, 'r',  encoding='utf-8') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.insertions = defaultdict(int)\n",
    "        self.deletions = defaultdict(int)\n",
    "        self.replacements = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def update_statistics(self, given, fixed):\n",
    "        operations = Levenshtein.opcodes(given, fixed)\n",
    "        for op in operations:\n",
    "            name, i1, i2, j1, j2 = op[0], op[1], op[2], op[3], op[4]\n",
    "            if name == 'insert':\n",
    "                for c in fixed[j1:j2]:\n",
    "                    self.insertions[c] += 1\n",
    "            if name == 'delete':\n",
    "                for c in given[i1:i2]:\n",
    "                    self.deletions[c] += 1\n",
    "            if name == 'replace':\n",
    "                for c1, c2 in zip(given[i1:i2], fixed[j1:j2]):\n",
    "                    self.replacements[c1][c2] += 1\n",
    "\n",
    "    def calculate_weights(self):\n",
    "        self.calculate_replacement_weights()\n",
    "        self.deletion_weights = ErrorModel.calculate_list_weights(\n",
    "            self.deletions)\n",
    "        self.insertion_weights = ErrorModel.calculate_list_weights(\n",
    "            self.insertions)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_list_weights(dictionary):\n",
    "        list_frequencies_to_weights, default_weight = ErrorModel.prepare_weights(\n",
    "            dictionary.values())\n",
    "        list_weights = defaultdict(lambda: default_weight)\n",
    "        for k in dictionary.keys():\n",
    "            list_weights[k] = list_frequencies_to_weights[dictionary[k]]\n",
    "        return list_weights\n",
    "\n",
    "    def calculate_replacement_weights(self):\n",
    "\n",
    "        replacement_frequencies_to_weights, default_weight = ErrorModel.prepare_weights(\n",
    "            (flatten_dictionary(self.replacements)))\n",
    "\n",
    "        self.replacement_weights = defaultdict(\n",
    "            lambda: defaultdict(lambda: default_weight))\n",
    "        for k1, v in self.replacements.items():\n",
    "            for k2 in v.keys():\n",
    "                self.replacement_weights[k1][\n",
    "                    k2] = replacement_frequencies_to_weights[self.replacements[k1][k2]]\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_weights(values):\n",
    "        list_frequencies = np.sort(np.array(values))[::-1]\n",
    "        list_weights = np.log1p(list_frequencies).astype(float)[::-1]\n",
    "        list_frequencies_to_weights = {}\n",
    "        for i in range(len(list_frequencies)):\n",
    "            list_frequencies_to_weights[list_frequencies[i]] = list_weights[i]\n",
    "        default_weight = np.max(list_weights)\n",
    "        return list_frequencies_to_weights, default_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill  \n",
    "import pandas as pd\n",
    "from re import escape\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Levenshtein' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-baf1f30175bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mnumber_of_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfixed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0merror_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0merror_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-edb0effb1191>\u001b[0m in \u001b[0;36mupdate_statistics\u001b[1;34m(self, given, fixed)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgiven\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moperations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLevenshtein\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopcodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgiven\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moperations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Levenshtein' is not defined"
     ]
    }
   ],
   "source": [
    "queries = read('queries_all.txt')\n",
    "queries = [q.split('\\t') for q in queries]\n",
    "# queries_lod = []\n",
    "original_queries = []\n",
    "fixed_queries = []\n",
    "for q in queries:\n",
    "    if len(q) == 2:\n",
    "        original_queries.append(q[0])\n",
    "        fixed_queries.append(q[1])\n",
    "        # queries_lod.append({'query': q[0], 'fixed': q[1], 'needs_fix': True})\n",
    "    else:\n",
    "        original_queries.append(q[0])\n",
    "        fixed_queries.append(q[0])\n",
    "        # queries_lod.append({'query': q[0], 'fixed': q[0], 'needs_fix': False})\n",
    "# queries_df = pd.DataFrame(queries_lod)\n",
    "\n",
    "# SPLIT\n",
    "\n",
    "punctuation = escape(punctuation)\n",
    "\n",
    "# fixed_queries_to_words =\n",
    "# queries_df['fixed'].str.decode('utf-8').replace('[' + punctuation +']',\n",
    "# '', regex=True).str.split()\n",
    "fixed_queries_to_words = pd.Series(fixed_queries).replace('[' + punctuation + ']', '', regex=True).str.split()\n",
    "fixed_words = flatten_list(fixed_queries_to_words)\n",
    "frequency_dictionary_of_fixed_words = Counter(fixed_words)\n",
    "\n",
    "# original_queries_to_words =\n",
    "# queries_df['query'].str.decode('utf-8').replace('[' + punctuation +']',\n",
    "# '', regex=True).str.split()\n",
    "original_queries_to_words = pd.Series(original_queries).replace('[' + punctuation + ']', '', regex=True).str.split()\n",
    "original_words = flatten_list(original_queries_to_words)\n",
    "# frequency_dictionary_of_original_words = Counter(original_words)\n",
    "\n",
    "# CALCULATE ERROR MODEL\n",
    "\n",
    "error_model = ErrorModel()\n",
    "\n",
    "for original, fixed in zip(original_queries_to_words, fixed_queries_to_words):\n",
    "    number_of_words = min(len(original), len(fixed))\n",
    "    for i in range(number_of_words):\n",
    "        error_model.update_statistics(original[i], fixed[i])\n",
    "\n",
    "error_model.calculate_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
