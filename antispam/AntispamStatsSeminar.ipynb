{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Описание **"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Построить графики распределения в спам и не спам множествах следующих признаков:\n",
    "\n",
    "1\tКоличество слов на странице\n",
    "2\tСредняя длинна слова\n",
    "3\tКоличество слов в заголовке страниц (слова в теге <html><head><title> Some text </title>)\n",
    "4\tКоличество слов в анкорах ссылок (<html><body><a> Some text </a>)\n",
    "5\tКоэффициент сжатия\n",
    "\n",
    "Нужно посчитать статистику минимум по трем признакам и обязательно сделать для 1-го и 2-го признаков\n",
    "\n",
    "И отправить первое решение в соревнование https://kaggle.com/join/antispam_infopoisk\n",
    "На основании одного из указанных выше признаков попытаться разделить мн-во, так чтобы score в соревновании был больше 0.55\n",
    "\n",
    "При выполнении всех этих условия в течении семинара +1 балл к ДЗ\n",
    "\n",
    "Описание ДЗ и правил выставления за него баллов в https://inclass.kaggle.com/c/antispam-infopoisk  \n",
    "Сроки ДЗ уточнить у преподователя - обычно 2 недели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vkrin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import scipy\n",
    "import binascii\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import base64\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gzip\n",
    "import pymorphy2\n",
    "import zlib\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import importlib\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import xgboost\n",
    "import zipfile\n",
    "import gzip\n",
    "import string\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from io import BytesIO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import namedtuple\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from bs4.element import Comment\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing.pool import ThreadPool\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_NUM = 100\n",
    "import logging\n",
    "importlib.reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "\n",
    "def trace(items_num, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d\" % items_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Утилиты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Декораторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_utf8(text):\n",
    "    if isinstance(text,  str): text = text.encode('utf8')\n",
    "    return text\n",
    "\n",
    "def convert2unicode(f):\n",
    "    def tmp(text):\n",
    "        if not isinstance(text,  str): text = text.decode('utf8')\n",
    "        return f(text)\n",
    "    return tmp\n",
    "\n",
    "def convert2lower(f):\n",
    "    def tmp(text):        \n",
    "        return f(text.lower())\n",
    "    return tmp\n",
    "\n",
    "#P.S. Декораторы могут усложнять отладку, так что от них вполне можно отказаться и воспользоваться copy-paste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извлечение текста из html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Извлечение текста при помощи встроенных модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import re\n",
    "\n",
    "###Извлечение текста из title можно вписать сюда\n",
    "\n",
    "class TextHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self._text = []\n",
    "        self._title = \"\"\n",
    "        self._in_title = False\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        text = data.strip()\n",
    "        if len(text) > 0:\n",
    "            text = re.sub('[ \\t\\r\\n]+', ' ', text)\n",
    "            self._text.append(text + ' ')\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'p':\n",
    "            self._text.append('\\n\\n')\n",
    "        elif tag == 'br':\n",
    "            self._text.append('\\n')\n",
    "        elif tag == 'title':\n",
    "            self._in_title = True\n",
    "\n",
    "    def handle_startendtag(self, tag, attrs):\n",
    "        if tag == 'br':\n",
    "            self._text.append('\\n\\n')\n",
    "\n",
    "    def text(self):\n",
    "        return ''.join(self._text).strip()\n",
    "\n",
    "@convert2lower\n",
    "def html2text_parser(text):\n",
    "    parser = TextHTMLParser()\n",
    "    parser.feed(text)\n",
    "    return parser.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]'] or isinstance(element, Comment):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    \n",
    "    links = []\n",
    "    links_t=''\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        links.append(link.get('href'))\n",
    "        links_t+=(' '+link.get_text())\n",
    "    \n",
    "    return ' '.join(visible_texts), links, links_t\n",
    "\n",
    "def tokenize_me(file_text):\n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "    tokens = [morph.parse(i.lower())[0].normal_form for i in tokens if ( i not in string.punctuation )]\n",
    " \n",
    "    stop_words = stopwords.words('russian')+stopwords.words('english')\n",
    "    stop_words.extend(map(lambda x: x, \n",
    "                          ['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'на']))\n",
    "    tokens = [i for i in tokens if ( i not in stop_words )]     \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnz = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def norm(text):\n",
    "    restxt=''\n",
    "    text=text.lower()\n",
    "    for token in tnz.tokenize(text):\n",
    "        if (token not in stopwords.words('Russian')) and (token != '©') and (token.isdigit() == False) \\\n",
    "        and isnotpunct(token) and (len(token)>1) and (token != '…') and \\\n",
    "        (token != '»') and (token != '«') and (token != '—') and hasnotdigits(token):\n",
    "            token = morph.parse(token)[0].normal_form\n",
    "            restxt=restxt+\" \"+token   \n",
    "    return restxt \n",
    "def isnotpunct(strk):\n",
    "    for s in strk:\n",
    "        if s in punctuation:\n",
    "            return False\n",
    "    return True  \n",
    "\n",
    "def hasnotdigits(token):\n",
    "    for s in token:\n",
    "        if s in '0123456789':\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Извлечение текста при помощи дополнительных библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2text_bs(raw_html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    \"\"\"\n",
    "    Тут производится извлечения из html текста\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    [s.extract() for s in soup(['script', 'style'])]\n",
    "    return soup.get_text()\n",
    "\n",
    "def html2text_title(raw_html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    \"\"\"\n",
    "    Тут производится извлечения из html текста\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    s=soup.find_all('title')\n",
    "    if s:\n",
    "        return tokenize_me(s[0].get_text())\n",
    "    return ''\n",
    "\n",
    "def html2text_ss(raw_html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    \"\"\"\n",
    "    Тут производится извлечения из html текста\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    links=[]\n",
    "    texts=''\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        links.append(link['href'])\n",
    "        for text in link.get_text().split():\n",
    "            texts=texts+' '+text\n",
    "    return ' '.join(links),' '.join(tokenize_me(texts))\n",
    "\n",
    "def html2text_bs_visible(raw_html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    \"\"\"\n",
    "    Тут производится извлечения из html текста, который видим пользователю\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")    \n",
    "    [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "    return ' '.join(tokenize_me(soup.get_text()))\n",
    "\n",
    "def html2text_boilerpipe(raw_html):\n",
    "    import boilerpipe\n",
    "    \"\"\"\n",
    "    еще одна библиотека очень хорошо извлекающая именно видимый пользователю текст,\n",
    "    но она завязана на java\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выбираем какой метод для конвертации html в текст будет основным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html2text = html2text_bs\n",
    "html2text = html2text_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методы для токенизации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@convert2lower\n",
    "@convert2unicode\n",
    "def easy_tokenizer(text):\n",
    "    word = str()\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum(): word += symbol\n",
    "        elif word:\n",
    "            yield word\n",
    "            word = str()\n",
    "    if word: yield word\n",
    "\n",
    "PYMORPHY_CACHE = {}\n",
    "MORPH = None\n",
    "#hint, чтобы установка pymorphy2 не была бы обязательной\n",
    "def get_lemmatizer():\n",
    "    import pymorphy2\n",
    "    global MORPH\n",
    "    if MORPH is None: MORPH = pymorphy2.MorphAnalyzer()\n",
    "    return MORPH\n",
    "\n",
    "@convert2lower\n",
    "@convert2unicode\n",
    "def pymorphy_tokenizer(text):\n",
    "    global PYMORPHY_CACHE\n",
    "    for word in easy_tokenizer(text):\n",
    "        word_hash = hash(word)\n",
    "        if word_hash not in PYMORPHY_CACHE:\n",
    "            PYMORPHY_CACHE[word_hash] = get_lemmatizer().parse(word)[0].normal_form            \n",
    "        yield PYMORPHY_CACHE[word_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основная функция, которая вызывается для преобразования html в список слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2word(raw_html, to_text=html2text, tokenizer=easy_tokenizer):\n",
    "    return tokenizer(to_text(raw_html).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Рассчет финальных метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(a, b):\n",
    "    if a == 0: return 0.0\n",
    "    elif b == 0: return 0.0\n",
    "    else: return a/b\n",
    "\n",
    "def calculate_metrics(predictions, threshold):    \n",
    "    \"\"\"\n",
    "    Функция подсчета метрик\n",
    "    Параметры\n",
    "    predictions - ранки по документам\n",
    "    threshold  - порог для метрик\n",
    "    \"\"\"\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    for (url_id, mark, url, prediction) in predictions:        \n",
    "        mark_predict = prediction > threshold\n",
    "\n",
    "        if mark_predict:                     \n",
    "            if mark_predict == mark: true_positive += 1\n",
    "            else: false_positive += 1                    \n",
    "        else:                     \n",
    "            if  mark_predict == mark: true_negative += 1\n",
    "            else: false_negative += 1\n",
    "\n",
    "    class_prec  = safe_divide(true_positive, true_positive + false_positive)\n",
    "    class_recall = safe_divide(true_positive, true_positive + false_negative)\n",
    "        \n",
    "    class_F1 = safe_divide(2 * class_prec * class_recall, class_prec + class_recall)\n",
    "    \n",
    "    \n",
    "    not_class_prec = safe_divide(true_negative, true_negative + false_negative)\n",
    "    not_class_recall = safe_divide(true_negative, true_negative + false_positive)\n",
    "    \n",
    "    not_class_F1 = safe_divide(2 * not_class_prec * not_class_recall, not_class_prec + not_class_recall)\n",
    "    \n",
    "    return ( (class_prec, class_recall, class_F1), (not_class_prec, not_class_recall, not_class_F1) )\n",
    "\n",
    "def arange(start, stop, step):\n",
    "    cur_value = start\n",
    "    while True:\n",
    "        if cur_value > stop: break\n",
    "        yield cur_value\n",
    "        cur_value += step\n",
    "\n",
    "def plot_results(docs, min_threshold=-1, max_threshold=1, step=0.1, trace=False):\n",
    "    x = []\n",
    "    y_p = []\n",
    "    y_n = []\n",
    "    docs_predictions = classifier.predict_all(docs)\n",
    "    for threshold in arange(min_threshold, max_threshold, step):\n",
    "        r = calculate_metrics(docs_predictions, threshold)\n",
    "        x.append(threshold)\n",
    "        y_p.append(r[0])\n",
    "        y_n.append(r[1])        \n",
    "        if trace: \n",
    "            print ('threshold %s',  threshold)\n",
    "            print ('\\tclass_prec %s, class_recall %s, class_F1 %s',  r[0])\n",
    "            print ('\\tnot_class_prec %s, not_class_recall %s, not_class_F1 %s',  r[1])\n",
    "            print ('\\t\\tMacroF1Mesure %s',  ((r[0][2] + r[1][2])/2))\n",
    "    plot_stats(x, y_p, \"Class Result\")\n",
    "    plot_stats(x, y_n, \"Not class Result\")    \n",
    "\n",
    "\n",
    "def plot_stats(x, y, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    prec, = plt.plot( x, \n",
    "                     [k[0] for k in y], \"r\", label='Precision', \n",
    "                     linewidth=1)\n",
    "    accur, = plt.plot( x, \n",
    "                      [k[1] for k in y], \"b\", label='Recall',\n",
    "                      linewidth=1)\n",
    "    f1, =    plt.plot( x, \n",
    "                      [k[2] for k in y], \"g\", label='F1',\n",
    "                      linewidth=1)\n",
    "    plt.grid(True)\n",
    "    plt.legend(handles=[prec, accur, f1])\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(url, html_data,title=False):\n",
    "    text, links, ankors = text_from_html(html_data)\n",
    "    text=tokenize_me(text)\n",
    "    ankors=tokenize_me(ankors)\n",
    "    all_text=tokenize_me(text) + ' '.join(links)+' '+tokenize_me(ankors)\n",
    "    words = text.split()\n",
    "    words_num = len(words)\n",
    "    ankors_num=len(ankors.split())\n",
    "    urls_num=len(links)\n",
    "    if words_num>0:\n",
    "        uniq=len(set(words))/words_num\n",
    "    else:\n",
    "        uniq=0\n",
    "    len_url=1/len(url)\n",
    "    compression_level = len(gzip.compress(html_data))/len(html_data)\n",
    "    return [len(words), compression_level, all_text, uniq, urls_num,len_url, ankors_num, ankors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url'])#, 'features'])\n",
    "\n",
    "def load_csv(input_file_name, calc_features_f):    \n",
    "    \"\"\"\n",
    "    Загружаем данные и извлекаем на лету признаки\n",
    "    Сам контент не сохраняется, чтобы уменьшить потребление памяти - чтобы\n",
    "    можно было запускать даже на ноутбуках в классе\n",
    "    \"\"\"\n",
    "    \n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name,encoding='utf8')  as input_file:            \n",
    "        headers = input_file.readline()\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.decode('utf8').strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            mark = bool(int(parts[1]))                    \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)\n",
    "            #features = calc_features_f(url, html_data)            \n",
    "            yield DocItem(url_id, mark, url)#, np.array(features))            \n",
    "                \n",
    "        trace(i, 1)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Обрабатываем входной файл **\n",
    "<br>\n",
    "Формат - поля разделенные табуляциями\n",
    "<br>\n",
    "0 - идентификатор документа\n",
    "<br>\n",
    "1 - метка класса 0 - не спам, 1 - спам\n",
    "<br>\n",
    "2 - урл документа\n",
    "<br>\n",
    "3 - документ в кодировке base64\n",
    "\n",
    "Выходной формат - массив кортежей вида\n",
    "(doc_id, is_spam, url, html_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:52:03 INFO:Complete items 00000\n",
      "00:52:03 INFO:Complete items 00100\n",
      "00:52:04 INFO:Complete items 00200\n",
      "00:52:04 INFO:Complete items 00300\n",
      "00:52:04 INFO:Complete items 00400\n",
      "00:52:04 INFO:Complete items 00500\n",
      "00:52:04 INFO:Complete items 00600\n",
      "00:52:04 INFO:Complete items 00700\n",
      "00:52:04 INFO:Complete items 00800\n",
      "00:52:04 INFO:Complete items 00900\n",
      "00:52:04 INFO:Complete items 01000\n",
      "00:52:04 INFO:Complete items 01100\n",
      "00:52:04 INFO:Complete items 01200\n",
      "00:52:04 INFO:Complete items 01300\n",
      "00:52:05 INFO:Complete items 01400\n",
      "00:52:05 INFO:Complete items 01500\n",
      "00:52:05 INFO:Complete items 01600\n",
      "00:52:05 INFO:Complete items 01700\n",
      "00:52:05 INFO:Complete items 01800\n",
      "00:52:05 INFO:Complete items 01900\n",
      "00:52:05 INFO:Complete items 02000\n",
      "00:52:05 INFO:Complete items 02100\n",
      "00:52:05 INFO:Complete items 02200\n",
      "00:52:05 INFO:Complete items 02300\n",
      "00:52:05 INFO:Complete items 02400\n",
      "00:52:05 INFO:Complete items 02500\n",
      "00:52:05 INFO:Complete items 02600\n",
      "00:52:05 INFO:Complete items 02700\n",
      "00:52:06 INFO:Complete items 02800\n",
      "00:52:06 INFO:Complete items 02900\n",
      "00:52:06 INFO:Complete items 03000\n",
      "00:52:06 INFO:Complete items 03100\n",
      "00:52:06 INFO:Complete items 03200\n",
      "00:52:06 INFO:Complete items 03300\n",
      "00:52:06 INFO:Complete items 03400\n",
      "00:52:06 INFO:Complete items 03500\n",
      "00:52:06 INFO:Complete items 03600\n",
      "00:52:06 INFO:Complete items 03700\n",
      "00:52:06 INFO:Complete items 03800\n",
      "00:52:06 INFO:Complete items 03900\n",
      "00:52:06 INFO:Complete items 04000\n",
      "00:52:06 INFO:Complete items 04100\n",
      "00:52:07 INFO:Complete items 04200\n",
      "00:52:07 INFO:Complete items 04300\n",
      "00:52:07 INFO:Complete items 04400\n",
      "00:52:07 INFO:Complete items 04500\n",
      "00:52:07 INFO:Complete items 04600\n",
      "00:52:07 INFO:Complete items 04700\n",
      "00:52:07 INFO:Complete items 04800\n",
      "00:52:07 INFO:Complete items 04900\n",
      "00:52:07 INFO:Complete items 05000\n",
      "00:52:07 INFO:Complete items 05100\n",
      "00:52:07 INFO:Complete items 05200\n",
      "00:52:08 INFO:Complete items 05300\n",
      "00:52:08 INFO:Complete items 05400\n",
      "00:52:08 INFO:Complete items 05500\n",
      "00:52:08 INFO:Complete items 05600\n",
      "00:52:08 INFO:Complete items 05700\n",
      "00:52:08 INFO:Complete items 05800\n",
      "00:52:08 INFO:Complete items 05900\n",
      "00:52:08 INFO:Complete items 06000\n",
      "00:52:08 INFO:Complete items 06100\n",
      "00:52:08 INFO:Complete items 06200\n",
      "00:52:08 INFO:Complete items 06300\n",
      "00:52:08 INFO:Complete items 06400\n",
      "00:52:08 INFO:Complete items 06500\n",
      "00:52:09 INFO:Complete items 06600\n",
      "00:52:09 INFO:Complete items 06700\n",
      "00:52:09 INFO:Complete items 06800\n",
      "00:52:09 INFO:Complete items 06900\n",
      "00:52:09 INFO:Complete items 07000\n",
      "00:52:09 INFO:Complete items 07043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TRAIN_DATA_FILE  = 'kaggle_train_data_tab.csv.gz'\n",
    "\n",
    "train_docs =list(load_csv(TRAIN_DATA_FILE, calc_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_=scipy.sparse.load_npz('train.npz')\n",
    "X_test_=scipy.sparse.load_npz('test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs=np.load('train_docs.npy',allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from shutil import copyfileobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(train_docss):\n",
    "    for doc in train_docss:\n",
    "        yield doc[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=0.001, use_idf=True, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=0.001, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True,\n",
       "                vocabulary={'00', '000', '01', '01 2010', '01 2011', '02',\n",
       "                            '02 2010', '02 2011', '03', '03 2010', '03 2011',\n",
       "                            '04', '04 08', '04 11', '04 2010', '04 2011', '05',\n",
       "                            '05 2010', '06', '06 2010', '07', '07 04',\n",
       "                            '07 2010', '08', '08 04', '08 04 2011', '08 2010',\n",
       "                            '09', '09 2010', '10', ...})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(generator(train_docss=train_docs+test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vectorizer.fit_transform(generator(train_docss=train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'этаж', 'солт', '13', 'двадцать', 'израиль', 'рёрєр', 'как', 'утилит', 'метро', 'приложение', 'e5', 'приходиться', '81', 'настоящий', 'http', 'uslugi', 'умереть', 'старик', 'туда', 'тёмный', 'ночью', 'prv', '72', 'согласно', 'open', 'база', 'безопасность', 'сколько', 'камаз', 'итог', 'ширина', 'e8', 'факс', 'омск', 'ноябрь', 'детский', 'сњс', 'красноярск', 'key', 'анекдот', 'вернуться', 'легко', 'выполнить', 'быстрый', 'октябрь', 'реализация', 'боев', 'кредитный', 'появляться', 'рёсџ', 'ха', 'tds', 'западный', 'представитель', 'мтс', 'бабушка', 'сотрудничество', 'ростов', 'склад', 'препарат', 'шоу', 'этап', 'витамин', 'порно', 'сеть', 'металл', 'tiu', 'category', 'разрешение', 'авто', 'процесс', 'актёр', '22', 'on', 'активный', 'отдельный', 'упражнение', 'обратный', 'позволить', 'врач', 'рєс', 'кремлёвский', 'знак', '09', '2005', '58', 'самостоятельно', 'build', 'краснодар', 'корень', 'почувствовать', 'leave', 'модель', 'удалить', '77', 'сђс', 'приехать', 'восстановление', 'igrat', 'сюда', 'выражение', 'почка', 'суд', 'иначе', 'иностранный', 'users', 'top', 'подписка', 'articles', 'бытовой', 'разработка', 'надежда', 'открыть', '63', 'сюжет', 'телевидение', 'школьник', 'v6', 'заявка', 'пользоваться', 'вместе', 'показатель', 'долго', 'охота', 'контроль', 'челябинск', 'kiev', 'секс', 'телец', 'центральный', 'ipod', 'калория', 'брак', 'курсовой', 'соглашение', 'сиська', 'сќс', 'негритянка', 'политика', 'среда', 'sms', '150', 'прежде', 'жопа', 'стен', 'персонаж', 'ради', 'запретить', 'растение', '2012', 'продажа', 'спб', 'небо', 'особенность', 'поздний', 'беларусь', 'кто', 'комплекс', 'фотогалерея', 'gde', 'получить', 'запись', 'голова', 'igry', 'режиссёр', 'register', 'вверх', 'игровой', 'репутация', 'hit', 'физический', 'youtube', '120', 'знаешь', 'регистрация', 'спорт', 'dvd', 'правовой', 'конкурс', 'китай', 'душить', '1998', 'флёша', '37', 'платформа', 'музыка', 'граница', '91', 'классический', 'способ', 'погода', 'некоторый', 'экран', 'приключение', 'de', 'юлий', 'серьёзный', 'вариант', 'воздействие', 'директор', '60', 'благодаря', 'никак', 'самар', 'view', 'офис', 'en', 'page', 'суббота', 'оформление', 'верить', 'матч', 'тв', 'го', 'aspx', 'сканер', 'e2', 'руководство', 'мастер', 'map', 'словарь', 'тепло', 'российский', 'войско', 'композиция', 'текущий', 'cx', 'letitbit', 'v5', 'file', 'warcraft', '45', 'вакансия', 'собрать', 'it', 'facebook', 'образовательный', 'красноярский', 'стать', 'находиться', 'объём', 'нормальный', 'рак', 'всякий', 'изменение', 'đľń', 'gps', 'обновить', 'su', 'поверхность', 'сфера', 'действительно', 'декабрь', 'bb', 'kp', 'месяц', 'ru', 'помогать', 'город', 'огромный', 'weather', '99', 'вызвать', 'мужской', 'đžń', 'почтовый', 'отчество', 'санкт', 'sid', 'чехол', 'цена', 'рейтинг', 'станция', 'улыбка', 'правка', 'сборник', 'microsoft', 'rus', 'торговый', 'list', '52', 'satrip', 'одесса', 'ремонт', 'create', 'сайтhttp', 'форма', 'кресло', 'кухня', 'саратов', 'ру', 'американский', 'cid', 'де', '2011', 'оптом', 'мелкий', 'камера', 'бумага', 'услышать', 'желать', 'установить', 'ii', 'зимний', 'бар', 'борьба', 'удобный', 'молоденький', 'eng', 'любовный', 'зуб', 'код', 'компания', 'переход', 'новость', 'дима', 'думать', 'by', '12', 'расположение', 'статистика', 'бежать', 'прочий', 'psp', 'рецепт', 'около', 'татьяна', 'выше', 'intim', 'попросить', 'зайти', 'мужик', 'bf', 'search', 'главный', 'минута', 'qip', 'рубрика', 'top100', 'v7', 'инструкция', 'cookie', 'd1', 'устройство', 'установленный', 'жена', 'sm', 'moskvi', 'попробовать', 'бассейн', 'дом', 'files', 'возникать', 'материал', '38', 'майк', 'рост', 'занимать', 'пора', 'wordpress', 'cw', 'понять', 'возможность', 'методика', 'сентябрь', 'солдат', 'называться', '110mb', 'пространство', 'ложка', 'live', 'подробность', 'иной', '2006', 'агент', 'больница', 'лидер', 'index', 'telefonniy', 'робот', 'книга', 'house', 'юный', 'корпоративный', 'дилер', 'e0', 'inf', 'должность', 'кабель', 'сђрѕр', 'polovogo', 'обнаружить', 'заявить', 'телевизор', 'желание', 'случайный', 'сантиметр', 'музыкальный', 'elements', 'новогодний', 'afisha', 'converter', 'рѕрір', 'посёлок', 'немного', 'свой', 'пётр', 'fifa', 'таблетка', 'стараться', 'др', 'справа', 'viewtopic', 'tag', 'число', 'особенно', 'летний', 'liveinternet', 'фитнес', 'прикольный', 'основной', 'рѕ', 'оставлять', 'самый', 'япония', 'определить', 'поддерживать', 'должный', 'личный', '1999', 'раздел', 'организм', 'wow', 'about', 'чувствовать', 'кинотеатр', 'федеральный', 'отправиться', 'начинать', 'важный', 'оказывать', 'ввести', '56', 'князь', 'keyword', 'взгляд', 'старший', 'практический', 'us', 'хотеться', 'кредит', 'открытие', '79', 'кабинет', '65', 'газ', 'экономика', 'домой', 'href', 'journal', '55', 'муниципальный', 'впервые', 'потребитель', 'украина', 'чита', 'направление', 'gf100txr4', 'готовый', 'университет', 'hyundai', 'вологда', 'европейский', '18', 'online', 'житель', 'юридический', 'магазин', 'автокад', 'service', 'например', 'развитие', 'забыть', 'калининград', 'showuser', 'jurproffi', 'сначала', 'телефонный', 'путешествие', 'среди', 'edu', 'черта', 'bluetooth', 'против', 'деньга', 'ссылка', 'чтение', 'ст', 'radeon', 'ziza', 'литература', 'модератор', 'предок', 'cat', 'рє', 'литр', 'дама', 'сервис', 'zip', 'ваза', 'кб', 'фирма', 'eb', 'состояться', 'реальный', 'mazda', 'точно', 'friends', 'шея', 'оставаться', 'аэропорт', 'продолжать', 'рѕрёс', 'gt', 'запад', 'высота', 'таковой', 'приходить', 'выход', 'colordepth', 'память', 'гражданин', 'недвижимость', 'максим', 'электрический', 'степень', 'действие', 'лишь', 'vin', '3d', 'персонал', 'понятие', 'uvelichenie', 'ресурс', 'правильно', '80', 'следующий', 'ниже', 'баба', '2003', 'nfs', 'чертёж', 'решить', 'пакет', 'поиск', 'phpsessid', 'звуковой', 'контакт', 'вход', 'тур', 'downloader2', '66', 'wp', 'админ', 'наушник', 'no', 'новое', 'обмен', 'представление', 'профиль', 'рџрѕсђрѕрѕ', 'дверь', 'праститутка', 'диетолог', 'рассказывать', 'одежда', 'ход', 'март', 'русский', 'интимный', 'sex', 'музей', 'оренбург', 'победа', 'потолок', 'content', 'видеть', 'министерство', 'судьба', 'отечественный', 'отвечать', 'весы', 'стол', 'начальник', 'надпись', 'очередной', 'следовать', 'энергия', 'mobile', 'вид', 'вдоль', 'starcraft', 'вязание', 'ирина', 'вера', 'операция', 'жёсткий', 'прийти', 'help', 'долина', 'всё', 'франция', 'платный', 'go', 'october', 'гостиница', 'nokia', 'свет', 'далеко', 'foto', 'сквозь', 'год', 'bez', 'установка', 'bml', 'параметр', 'доллар', 'мозг', 'инфекция', 'уникальный', 'являться', 'рќр', 'круг', 'pohudeniya', 'люба', 'ed', 'сила', 'рота', 'птица', 'масло', 'keygen', 'двигаться', 'onlayn', 'вконтакте', 'отсутствие', 'cохранить', 'герой', 'зеркало', 'коммуникатор', 'отделение', 'деревня', 'крайний', 'футболка', 'больший', '69', 'коммерческий', 'владимир', 'reply', 'будущее', 'версия', 'решение', 'видеокарта', 'николай', 'страховой', 'yahoo', 'usd', 'худой', 'друг', 'заведение', 'лена', 'сечь', 'программный', 'свободный', 'посмотреть', 'проблема', 'учреждение', 'проведение', 'назначение', 'наркотик', 'автомат', 'явление', 'обстоятельство', '48', 'лист', 'конференция', 'программа', 'анальный', 'смоленск', 'бросить', 'поезд', 'рі', 'лёгкий', 'учёный', 'получиться', 'любимый', '76', 'пройти', 'передать', 'свойство', 'comment', 'мвд', 'дон', '05', 'увеличение', 'mywordpress', 'рѕр', 'загрузить', 'качественный', 'сџ', 'нижний', 'дерево', 'губа', 'бедро', 'дмитрий', 'вышивка', 'название', 'кровь', 'производитель', 'искать', 'рѕрѕс', 'человеческий', 'возникновение', 'poisk', 'кстати', 'руссификатор', 'выбирать', 'член', 'армия', 'познакомиться', 'проводить', 'вещий', 'файл', 'сервер', 'блог', 'основный', 'рјрё', 'b1', 'что', 'смешной', 'андрей', 'кошка', 'предмет', 'xxx', 'завтра', 'логотип', 'front', 'yandex', 'hosting', 'iphone', 'каждый', 'копия', 'audio', 'individualki', 'способный', 'перейти', 'недавно', 'половый', 'оборудование', 'плюс', 'законодательство', 'касперский', 'скачать', 'работа', 'маршрут', 'алекс', 'снова', '2001', 'правило', 'целое', 'hotbox', 'приказ', 'загрузка', 'hd', 'схема', 'вскоре', 'рабочий', 'область', 'интересовать', 'скорее', 'территория', 'некий', 'обновление', 'мать', 'женщина', 'принцип', 'интересно', 'находить', 'обладать', 'ожидать', '2010http', 'взлом', 'японский', 'популярный', 'вс', 'black', 'набережная', 'ot', '1997', 'сделать', 'делать', 'internet', 'сын', 'защита', 'mode', 'офисный', 'несколько', 'обеспечивать', 'хотя', 'рїрѕс', 'причёска', 'dvdrip', 'bin', 'acer', 'office', 'боевик', 'ветер', 'занять', 'софт', 'казаться', 'private', '47', 'file4server2', 'chlen', 'newmail', 'инструмент', 'мм', 'простой', 'ребёнок', 'uvelicheniya', 'panasonic', 'дракон', 'район', 'тысяча', 'клиент', 'star', 'двигатель', 'linkstars', 'медиа', 'америка', 'кнопка', 'совместимость', 'журнал', 'ua', '3gp', 'старое', 'dr', 'цитировать', 'металлический', 'moskve', 'genius', 'стоять', 'релиз', 'vv', 'сказать', 'выбор', 'технический', 'жанр', 'малолеток', 'print', 'тема', '29', 'photo', 'храм', 'орган', 'трек', 'многий', 'применять', 'сумерки', 'strefa', 'отлично', 'вне', 'сѓрµрєсѓ', 'last', 'рїр', 'city', 'пятница', 'убить', 'asus', 'канал', 'платье', '96', 'ничто', 'слышать', 'расстояние', 'столица', 'значит', 'forum', 'english', 'голый', 'tags', 'международный', 'обращаться', 'консультация', 'moikrug', 'услуга', 'выпуск', 'спортивный', 'региональный', 'волна', 'xml', 'user', 'che', 'photoshop', 'соответствующий', 'plus', 'юбка', 'перевозка', 'рїсђр', 'отзыв', 'многие', 'беременность', 'предлагать', 'информационный', 'вопрос', '46', 'promodj', 'голос', 'sovmestimosti', 'грудь', 'mega', 'событие', 'intel', 'мир', 'достаточно', 'поставить', '97', 'евгений', 'медицинский', 'весна', 'studio', 'наталья', 'интерес', '1с', 'отправить', 'существо', 'рассказ', 'василий', 'слишком', 'суметь', '2002', 'культура', 'значительный', 'газета', 'подключение', 'проститутка', 'тольятти', 'инцест', 'роды', 'организация', 'таки', 'стандартный', '2007', 'длинный', 'ворота', 'сведение', 'стратегия', 'сумма', 'мировой', 'игрушка', 'блюдо', 'uni', 'рекомендовать', 'республика', 'рј', 'nova', 'фигура', 'рѕрѕрѕс', 'кино', 'рірѕс', 'относиться', 'положить', 'сразу', 'факт', 'идти', 'быстро', 'spylog', 'вероятно', 'письмо', 'диета', 'удаться', 'пизда', 'publ', 'деловой', 'spravochnik', 'изначально', 'link', 'рµс', 'in', 'is', 'свеча', 'разный', 'ref', 'сообщать', 'доктор', 'доход', 'наверное', 'pci', 'путь', 'прочее', 'очень', 'по', 'jpg', 'толстой', 'учебник', 'нахождение', 'напомнить', 'глаз', '16', 'звук', 'включить', 'страх', 'повышение', 'утром', 'тюмень', 'разработчик', 'сђрµр', 'гея', 'преимущество', 'infium', 'уровень', 'рµрѕс', 'topic', 'vol', 'постоянный', 'заставить', 'медицина', 'шкаф', 'движение', 'прочитать', 'начать', 'похожий', 'сохранить', 'v2', 'разместить', 'сетевой', 'рірѕ', 'включать', 'светлана', 'близнец', 'трахать', 'зарубежный', 'đžđ', 'ходить', 'pochtamt', 'поведение', 'кожа', 'урок', 'далее', 'понимать', 'небольшой', 'карта', 'design', '04', 'транспортный', 'поначалу', 'hotlog', 'write', 'пойти', 'получение', 'дать', 'дробный', 'сто', 'document', 'счастливый', '2008', 'касаться', 'свадебный', 'torrent', 'волос', 'powered', '23', 'государственный', 'роман', 'f2', 'расширить', 'закладка', 'строительный', 'парфюмерия', 'филиал', 'восток', 'продавать', '73', '35', 'мягкий', 'матка', 'михаил', 'dj', 'топ', 'цвета', 'норма', 'имя', 'любовь', 'улица', 'поднять', 'сперма', 'пусть', 'домен', 'практически', 'shlyuhi', 'формирование', 'validator', 'невеста', 'ка', 'чемпионат', 'длина', 'приобрести', 'рєр', 'нести', 'написать', '95', 'метка', 'агентство', 'принести', 'philips', 'именно', 'прямо', 'температура', '27', 'курс', 'заключение', 'механизм', 'мгновение', 'исполнитель', 'альбом', 'попытаться', '19', 'семь', 'потерять', 'оно', 'ещё', 'доска', 'xp', 'рірµс', 'убрать', 'разговор', 'пустой', '93', 'zodiaka', 'послать', 'lv', 'выезд', 'график', 'глядеть', 'пароль', 'сексуальный', 'попка', 'кома', 'chlena', 'archive', 'элемент', 'далёкий', 'znakomstva', 'владелец', '250', 'браузер', 'салон', 'практика', 'виртуальный', 'geforce', 'спонсор', 'see', 'medvopros', 'кислота', 'способность', 'крутой', 'дону', 'предложение', 'внутри', 'использование', 'gb', 'обслуживание', 'gmail', 'настолько', 'чаять', 'управление', 'вечер', 'подбор', 'январь', 'бизнес', 'персональный', 'научный', 'рѕсѓс', 'уходить', 'фонд', 'рамблер', 'прекрасный', 'ящик', 'чувство', 'матерь', 'февраль', 'гораздо', 'game', 'уважаемый', 'пункт', 'однажды', 'военный', 'купить', 'почта', 'the', 'воронеж', 'лишний', 'эксплуатация', 'лента', 'размер', 'статья', 'часто', 'животный', '25', 'банк', 'тег', 'пока', 'хватать', 'крыша', 'all', 'аккаунт', 'fm', 'центр', 'принять', 'кемерово', '83', 'b0', 'аптека', 'подруга', 'вроде', 'отель', 'фотоархив', 'мероприятие', 'транс', 'казахстан', 'финансовый', 'va', 'эл', 'бесплатно', 'спин', 'лекарство', 'стул', 'национальный', 'обработка', 'документ', 'file4server5', 'олег', 'театр', 'вперёд', '41', 'добавить', 'внешний', 'днём', 'way', 'верхний', 'право', 'ангел', 'строка', 'цель', 'создать', 'kb', 'ухо', 'печь', 'порнофильм', 'планет', 'оплата', 'любить', 'принимать', 'delivery', 'сайт', 'ріс', 'описание', 'пёп', 'цвет', 'кофе', 'россия', 'moskvy', 'примерно', 'запчасть', 'ставка', 'сбор', 'b2', 'знакомый', 'sport', 'новокузнецк', 'обращение', 'сегодня', 'ведущий', 'екатеринбург', 'сперва', 'учебный', 'привести', 'may', 'naruto', 'определённый', 'автомобильный', 'турция', '31', '40', 'avto', '001webs', 'состав', 'открытка', 'привет', 'нога', 'аудио', 'мл', 'носить', 'зарядный', 'милый', 'защитить', 'кризис', 'нибудь', 'синдром', 'значение', 'referrer', 'лошадь', 'последний', 'сторона', 'đľ', 'указанный', 'скидка', 'рґр', '53', 'туризм', 'издательство', 'ближний', 'ежели', 'весёлый', 'лесбиянка', 'вещество', 'php', 'онлайн', 'естественный', 'поль', 'xn', 'рѕрёрµ', 'семинар', 'заказ', 'реакция', 'сѓ', 'рєрѕрјсѓс', 'комната', 'редактор', '14', 'просмотр', 'иметься', 'gigabyte', 'smotret', 'объявление', 'папа', 'секрет', 'учитель', 'был', 'великое', 'ночь', 'набор', 'wikipedia', 'конкретный', 'сђрµс', 'разработать', 'sytes', 'получать', 'хирургия', 'панель', 'встретить', 'цифровой', 'родиться', 'угол', 'час', 'осуществлять', 'здравоохранение', 'account', 'biz', 'донецк', 'прошлый', 'использоваться', 'нравиться', 'title', 'взять', 'строительство', 'twitter', 'открыться', 'hp', 'f1', 'classic', 'megalyrics', 'прошлое', 'танец', 'рјрѕр', 'восточный', 'анализ', 'кровать', 'банка', 'дача', 'quick', 'группа', '100', 'екатерина', 'нажать', 'алматы', 'информация', 'ya', 'зрение', 'реклама', 'билан', 'showtopic', 'рѕрѕр', 'рекламный', 'максимальный', 'существовать', 'специальный', 'содержание', 'base', 'посвятить', 'авторизация', 'kino', 'toyota', 'давно', 'ответ', 'aldebaran', 'шина', 'партнёр', 'современный', '84', 'помощь', 'платёж', 'рјр', 'оператор', 'роль', 'немой', 'opel', 'размещение', 'использовать', 'река', 'условие', 'чей', 'лесби', 'com', 'meta', 'старый', 'питание', 'email', 'website', 'произойти', 'eminem', '500', 'символ', 'кольцо', 'контактный', 'депутат', 'елена', 'методический', 'работник', 'адресный', 'znakomstv', 'солнечный', 'пожалуйста', 'обои', 'бюджет', 'союз', 'страна', 'который', 'красота', 'досуг', 'sony', 'продать', 'escape', 'выбрать', 'лечение', 'английский', 'мера', 'называть', 'src', 'закрытый', 'max', '400', 'поддержка', 'bd', 'рамка', 'билайн', 'lenta', 'текст', 'жить', 'яндекс', 'спасибо', 'содержимый', 'психология', 'фактор', 'пт', 'рф', 'vip', 'post', 'лежать', 'рірёрґрµрѕ', 'правительство', 'skachat', 'обучение', 'южный', 'комедия', 'ладонь', 'дизайн', 'kz', 'минуть', '61', 'ce', 'нюша', 'запрос', 'бок', 'анкета', 'песнь', 'подросток', 'лёгкое', 'хозяйство', 'водитель', 'ford', 'песня', 'полный', 'pid', 'корпус', 'шлюха', 'мысль', '3a', 'совершенно', 'звонить', 'тест', 'mb', 'рис', 'read', 'помочь', 'wiki', 'cc', 'линия', 'mafia', 'заболевание', 'мб', 'компьютер', 'художественный', 'подготовка', 'настоящее', 'возле', 'происходить', 'председатель', 'конец', 'чистый', 'выполнять', 'данный', 'билет', 'check', 'надеяться', 'prostitutki', 'пить', 'mitsubishi', 'палец', 'войти', 'изделие', 'август', 'code', 'возникнуть', 'выйти', 'промышленный', 'собственный', 'задача', 'образование', '51', 'рѕрёсџ', 'взрослый', 'клетка', 'доступ', 'объявить', 'образ', 'info', 'обеспечение', 'особый', 'сравнить', 'xfn', 'пенза', 'narod2', 'pro', '90', 'forex', 'дальнейший', 'случиться', 'пять', '24', 'super', 'nm', 'обычно', 'выполнение', 'ноутбук', 'лицо', 'характеристика', 'интерьер', 'место', 'давать', 'достигнуть', 'основание', 'структура', 'пожаловаться', 'испытывать', 'border', 'утро', 'feedback', 'megashara', 'ряд', 'документальный', 'fam', 'налог', 'login', 'action', 'дополнительный', 'рірѕр', 'команда', 'искусство', 'ранний', 'челны', 'буква', 'seks', 'плат', 'server', 'best', 'пытаться', 'кв', 'ассоциация', 'молодёжный', 'замок', 'вслед', 'расшифровка', 'мясо', 'столько', 'dunpol', 'рµр', 'маркетинг', 'blog', '43', 'links', 'сѓс', 'успеть', 'источник', 'смс', 'screen', 'bc', 'наиболее', 'бесплатный', 'news', 'муж', 'острый', 'ответить', 'коляска', 'оказаться', 'метр', 'ericsson', 'ткань', 'затем', 'гарантия', 'пр', 'вкус', 'оставить', 'акт', 'групповой', 'сми', 'вино', 'твой', 'спать', 'auto', 'видно', 'oueb', 'сон', 'семья', 'сотрудник', 'помнить', '495', 'share', 'отец', 'администратор', 'jp', 'вод', 'аватар', 'модный', 'предыдущий', 'юрий', 'ec', 'означать', 'бывший', 'где', 'аська', 'free', 'web', 'гол', 'гарри', 'подумать', 'объект', 'полый', 'рука', 'администрация', 'поп', 'сердце', 'тёплый', 'realtek', 'груз', 'icq', 'сцена', 'транспорт', 'статус', 'подходить', 'goroskop', 'связь', '92', '2004', 'диагностика', 'usb', 'lib', 'блядь', 'кажется', 'иван', 'знать', 'хабаровск', 'минус', 'nf', 'здоровый', 'выпустить', 'причём', 'краска', 'cached', 'либо', 'горячее', 'компьютерный', 'железо', 'мультфильм', 'dlina', 'games', 'main', 'обратить', 'общий', 'очки', 'поставка', 'мечта', 'ba', 'kak', 'contact', 'радио', 'путаный', 'меню', 'тонкий', 'co', 'совет', 'анна', 'запомнить', 'ставить', 'морской', 'вызов', 'страшный', 'tools', '17', 'участие', 'ms', 'советский', 'наверх', 'леса', 'направить', 'строить', 'говорить', 'масса', 'кпк', 'исторический', 'дыхание', 'внимание', 'уход', 'отличный', 'аппарат', 'заработать', 'смотреть', 'процент', 'входить', 'чаща', 'местонахождение', 'зелёный', 'создание', 'дождь', 'фон', 'рёр', 'ник', 'академия', 'трудовой', 'вспомнить', 'индивидуалка', 'подобный', 'java', 'шесть', 'адвокат', 'держать', 'валюта', 'rock', 'массовый', 'президент', 'community', 'знакомство', 'сожаление', 'французский', '86', 'режим', 'зарегистрироваться', '10', 'малыш', 'добро', 'love', 'календарь', 'знаменитость', 'рѕрµ', 'казань', 'подойти', 'anime', '67', 'сбросить', 'рµсѓрїр', '98', 'поздравление', 'ucoz', 'битва', 'dieta', 'livejournal', 'волгоград', 'ул', 'свежий', 'полезный', 'club', 'пробить', 'её', 'секунда', 'ольга', 'обычный', 'произнести', 'специалист', 'rabota', 'moskva', 'приводить', 'новый', 'жёлтый', 'нарушение', 'звёздный', 'полиция', 'резко', 'возможный', 'козерог', 'modules', 'партия', 'windows', 'google', 'нея', 'сѓр', 'признать', 'сѓрєр', 'предел', 'сестра', 'лебедев', 'край', '2012342', 'звезда', 'торговля', 'учиться', 'pda', 'services', 'рё', 'фотография', 'момент', 'рок', 'играть', 'никакой', 'водолей', 'canon', 'зодиак', 'форум', 'доступный', 'расписание', 'составить', 'daily', 'двое', 'исполнительный', 'остальной', 'праздник', 'обзор', 'киевский', 'width', 'goto', 'прикол', 'вместо', 'рґрёрµс', 'проект', 'ролик', 'мор', 'везде', 'дата', 'оао', 'назад', '71', 'ранее', 'кадр', 'обсудить', 'девочка', 'болезнь', 'холодный', 'минусовка', 'отметить', 'процедура', 'передача', 'человек', '88', 'вокруг', 'юмор', 'диск', 'окно', 'admin', 'порядок', 'za', 'рљр', 'общественный', 'бой', 'финансы', 'мария', 'гонка', 'крупный', 'цитата', 'еда', 'шоссе', 'рір', 'ульяновск', 'сотовый', 'ошибка', 'известно', 'подмосковье', 'russia', 'blogs', 'москва', 'весь', 'стена', 'поток', 'антивирус', 'единый', 'mir', 'начаться', 'увеличить', 'растить', 'требование', '36', 'украинский', 'яркий', '94', 'telefonov', 'солнце', 'июль', 'стоимость', 'pr', 'указать', 'letter', 'b4', 'сочи', 'сообщить', 'рисунок', 'cb', 'mmorpg', 'миллион', 'массаж', 'цикл', 'приглашать', 'игра', 'правый', '87', 'счастие', 'экономический', 'помещение', 'images', 'народный', 'счёт', 'сообщество', 'lexus', 'professional', '26', 'зрелый', 'овен', 'обязательно', 'sk', '20', 'анатолий', 'требовать', 'менее', 'guest', 'класс', 'участвовать', 'кодекс', 'сеять', 'tracks', 'создавать', 'tk', 'ждать', 'vista', 'проспект', 'лицензия', 'зависимость', '2f', 'dousetsu', 'плечо', '82', 'ужас', 'пост', 'король', 'player', 'считаться', 'оценить', '01', 'навигация', 'tv', 'mail', 'технология', 'дорогой', 'лекарственный', 'толщина', 'действовать', 'никто', 'покупатель', 'рїрѕ', 'колено', 'рёрѕс', 'развивать', 'частное', 'рѕрјрѕр', 'картинка', 'увидеть', 'edition', 'план', 'оригинальный', 'архив', 'комиссия', 'отличаться', 'предназначить', 'продукция', 'load', 'конструкция', 'might', 'настройка', '75', 'косметика', 'слеза', 'задний', 'сѓсџ', 'na', 'налоговый', 'рµсѓс', '320', 'смочь', 'попасть', '78', 'нужно', 'интернетом', 'больной', 'построить', 'прогноз', 'чат', '49', 'марка', 'pages', 'определять', 'встать', 'просто', 'фотоаппарат', 'подход', 'предоставить', 'напоминать', 'бывать', 'земля', 'проезд', 'click', 'известный', 'мг', 'нужный', 'местный', 'горный', 'name', 'охрана', 'стихнуть', 'железный', 'добрый', 'ьhttp', 'also', 'familii', 'художник', 'besplatno', 'фото', 'проверка', 'комментировать', 'порнуха', 'собственность', 'мочь', 'проходить', 'страница', 'обязательный', 'занятие', 'group', 'менеджер', 'хит', 'сигнал', 'заголовок', 'развлечение', 'закрыть', 'сложный', 'рёрё', 'эксклюзивный', 'd0', 'комплект', 'do', 'faq', 'ответственность', 'покупка', 'фильм', 'жизнь', '9030', 'автобус', '32', 'kbps', 'доставка', 'плоский', 'кряк', 'стекло', 'изнасилование', 'справочный', 'признак', 'показать', 'средство', 'рѕрі', 'nayti', 'почему', 'денежный', 'показаться', 'настроение', 'городской', 'фестиваль', 'би', 'kreditovik', 'знание', 'очередь', 'year', 'квартира', 'материнский', '127a8dca241d597ea8de647a6d31b240', 'life', 'фабрика', 'справочник', 'красивый', 'александр', 'парк', 'возможно', 'рад', 'четыре', 'участник', 'snyat', 'необходимый', 'sponsor', 'возраст', '02', 'абсолютно', 'институт', 'любитель', '00', 'короткий', '08', 'однако', 'сша', 'сериал', 'издание', 'авторский', 'самоучитель', 'тайна', 'подписаться', 'asoter', 'равно', 'сказка', 'bmw', 'сильный', 'profile', 'pdf', 'киев', 'бельё', 'easy', 'вообще', 'комментарий', 'magic', 'тэг', 'ad', 'art', 'упасть', 'ожирение', 'автор', 'driver', 'поколение', 'родитель', 'рірµсђ', 'песок', '33', 'классный', 'рѕс', 'скорпион', 'представлять', 'связанный', 'рынок', '11', 'открывать', 'дорога', '1484', 'офицер', 'рїрѕр', 'приятный', 'подождать', 'резюме', 'height', 'лига', 'host', 'posts', 'господин', 'гора', 'позиция', 'autocad', 'lg', 'предоставлять', 'километр', 'похудение', 'тип', 'третье', 'интервью', 'наличный', 'дешёвый', 'ть', 'оружие', 'век', 'эротический', 'b5', 'девушка', 'идея', '28', 'lie', 'ruhttp', 'adobe', 'метод', 'linux', 'валентин', 'руководитель', 'часть', 'библиотека', 'flash', 'дело', 'министр', 'провести', 'предложить', 'попытка', 'показывать', 'великий', 'выходной', 'часы', 'реферат', 'dlya', 'монитор', 'comments', 'пособие', 'from', 'запах', 'translate', 'фантастика', 'рубль', 'какой', 'скачивание', 'ученик', 'огонь', 'portable', 'связаться', 'откуда', 'насос', 'сутки', '74', 'cureit', 'семейный', 'покинуть', 'след', 'мина', 'fallout', 'business', 'прямая', 'css', 'апрель', 'влияние', 'ключ', 'день', 'полностью', 'рѕрј', 'po', '34', 'отношение', 'единственный', 'рѕрёрє', 'spb', 'учёт', 'вновь', 'количество', 'myftp', 'обучать', 'shtml', 'народ', 'игорь', 'заметить', 'шлюшка', '128', 'redirect', 'nu', '59', 'виктор', 'изменить', 'rar', 'стрелец', 'final', 'опрос', 'госпожа', 'пользователь', 'словно', 'карьер', 'договор', 'адрес', 'принадлежать', 'heroes', 'левый', 'файловый', '85', 'журналист', 'состояние', 'img', 'служба', 'бдсм', 'imen', 'тело', 'ee', 'близкий', 'браузерный', 'тяжёлый', 'сравнение', 'расположить', 'тихо', 'ссср', 'энциклопедия', 'гостев', '06', 'радость', 'ярославль', 'печать', 'зарегистрировать', 'ижевск', 'минск', 'остановиться', 'север', 'речь', 'низкий', 'природа', 'двор', 'my', 'уфа', 'be', 'неделя', 'артикул', 'род', 'гей', 'необходимость', 'рўр', 'сезон', 'honda', 'похудеть', 'за', 'северный', 'sayt', 'telefon', 'рязань', 'таблица', 'посетитель', 'прибор', 'site', 'назвать', 'мебель', 'весьма', 'заниматься', 'cy', 'звонок', 'деревянный', '42', 'dvd5', 'из', 'столь', 'мощный', '21', '300', 'рѕрѕ', 'открытый', 'читатель', 'скоро', 'nissan', 'продукт', 'add', 'рµрѕрёсџ', 'пенис', 'широкий', 'появиться', 'домашний', 'слушать', 'телефон', 'консультант', 'shop', 'капитан', 'домодедово', 'andrey', 'война', 'ноготь', 'мама', 'логин', 'анал', 'дух', 'опыт', 'читать', 'ситуация', 'успех', 'закон', 'прислать', 'article', 'autodesk', 'изображение', 'jump', 'мобильник', 'cd', 'встреча', 'большой', 'список', 'v3', 'профессиональный', 'жертва', 'видио', 'metro', 'считать', 'электронный', 'государство', 'russian', 'студент', 'европа', 'на', 'соответствие', 'труба', 'angelfire', 'алексей', 'брать', 'харьков', 'ооо', 'наука', 'труд', 'рµрѕрёрµ', 'клуб', 'уметь', 'обувь', '812', 'soft', 'counter', 'rid', 'eu', 'org', 'опубликовать', 'w3', 'медленно', '70', 'интим', 'type', 'random', 'ati', 'увеличивать', 'состоять', 'богатый', 'колесо', 'net', 'барнаул', 'концерт', 'samsung', 'постоянно', 'акция', 'посетить', 'e4', 'свадьба', 'odnoklassniki', 'высокий', 'половина', 'произведение', 'вуз', '89', 'применение', 'подать', 'kupit', 'положение', 'система', 'feat', 'официальный', 'баня', 'ваш', '2009', 'торрент', 'сђр', 'хороший', 'просить', '64', 'mp3', 'листопад', 'окружающий', 'новинка', 'портал', 'китайский', 'lossless', 'оценка', 'гость', 'сесть', 'asp', 'id', 'абонент', 'основа', 'остров', 'продвижение', 'галерея', 'тверь', 'password', 'гороскоп', 'составлять', 'js', 'трава', 'питер', 'футбол', 'вниз', 'дневник', 'подарок', 'драма', 'проверить', 'land', 'depositfiles', 'фотка', 'rambler', 'эротика', 'gndmail', 'украшение', '50', 'машина', 'исполнение', 'воскресение', 'уведомление', 'средний', 'обсуждение', 'чёрный', 'сидеть', 'перевод', 'line', 'пациент', 'наблюдать', 'уйти', 'московский', 'рассказать', 'интересный', 'завод', 'костюм', 'свобода', 'исследование', 'крем', 'долгий', 'представить', 'заявление', '07', 'серия', 'vworld', 'путём', 'монтаж', 'денис', 'красный', 'наш', 'связать', 'редакция', 'dosug', 'эффект', 'опасный', 'хостинг', 'новосибирск', 'также', 'video', 'федерация', 'промышленность', 'участок', 'exe', 'десять', 'фраза', 'образец', 'постель', 'skype', 'студия', 'engine', 'мобильный', 'парфюм', 'аниме', 'ea', 'веб', 'narod', 'отчёт', 'penisa', 'самолёт', 'rss', 'верный', 'товар', 'происхождение', 'аренда', 'new', 'onlinetraff', 'мужчина', 'загранпаспорт', 'uvelichit', 'рµсђр', 'награда', 'цветок', 'прайс', 'чудо', 'встречаться', 'нос', 'url', 'снять', 'снег', 'стоялый', 'супруг', 'судебный', 'индивидуальный', 'дева', 'yc', 'писать', 'спросить', 'творческий', 'ck', 'конфиденциальность', 'сњ', 'то', 'вес', 'страхование', 'кп', 'cgi', 'passport', 'общество', 'аксессуар', 'изготовление', 'публикация', 'cs', 'приём', 'идеальный', 'правильный', 'глубокий', 'itemid', '1000', 'камень', 'закончить', 'сергей', 'скрытый', 'рєрё', 'сб', 'диван', 'враг', 'show', 'корабль', 'белый', 'рєрѕр', 'mp', 'msk', 'мало', 'определение', 'authors_alphabet', 'требоваться', 'автомобиль', 'act', 'рўрєр', 'универсальный', 'драйвер', 'борис', 'здоровье', 'маленький', 'боль', 'фотошоп', 'италия', 'номер', 'пн', 'любительский', 'казалось', 'sims', 'начинаться', '57', 'суп', 'испания', 'течение', 'период', 'остаться', 'управлять', 'archives', 'немецкий', 'мальчик', 'v4', 'выставка', 'станок', 'смысл', 'формат', 'гражданский', 'расчёт', 'рµрірѕ', 'html', 'площадь', 'русификатор', 'ресторан', 'хозяин', 'cnt', 'власть', 'выделить', 'собираться', 'впечатление', 'глубина', '800', 'рёс', '54', 'board', 'пластический', 'различный', 'вполне', 'заказать', 'школа', 'gov', 'redir', 'похудание', 'поэтому', 'подняться', 'трудоустройство', 'data', 'теория', 'глава', 'сњрѕс', 'catalog', 'следить', 'терапия', 'работать', 'выходить', 'слово', 'правда', 'афиша', 'подробный', 'внутренний', 'сухой', 'мышца', 'обязать', 'сад', 'появление', 'деятельность', 'коллекция', 'техника', 'социальный', 'пара', 'отдых', 'новгород', 'история', 'павел', 'большинство', 'удовольствие', 'gold', 'одновременно', 'v1', 'смерть', 'world', 'рµ', 'объяснить', '39', 'найти', 'баста', 'железнодорожный', 'игрок', 'cm', 'прийтись', 'брат', 'эксперт', 'вирус', 'рассылка', 'справка', 'корзина', '68', 'несмотря', 'владивосток', 'влагалище', 'предприятие', 'ga', 'пресс', 'кроме', 'вт', 'население', 'странный', 'автосалон', 'чт', 'марин', 'деталь', 'живой', 'поскольку', 'балл', 'pc', 'говорят', 'анастасий', 'элитный', 'блок', 'стандарт', 'королева', 'дочь', 'зал', 'мультик', 'стоить', 'петербург', 'видео', 'baza', 'зона', 'ванна', 'identity', 'июнь', 'причина', 'школьный', 'готовить', 'томск', 'дл', 'явно', 'фамилия', 'rihanna', 'сообщение', 'творчество', 'эффективность', '62', 'crack', 'юрист', '2000', 'каталог', 'зао', 'hotmail', '200', 'gta', 'плохо', 'сѓрїсђр', 'германия', 'мнение', 'отдать', 'шт', 'картина', 'живот', 'здравствовать', 'igri', 'events', 'церковь', 'становиться', 'cz', 'gallery', 'первое', 'обратиться', 'download', 'снижение', 'пермь', 'точка', 'золотой', 'задать', 'удар', 'рыба', 'мышь', 'множество', 'сумка', 'святой', 'покупать', 'иркутск', 'ум', 'срок', 'беременный', 'звать', 'едва', 'май', 'воздух', 'media', 'seksroliki', 'copyright', 'shluhi', 'узнать', 'сћс', '000', 'рґсђр', 'давление', 'категория', 'dll', 'вчера', 'функция', 'one', 'music', 'parameter', 'image', 'карман', 'худеть', 'скорость', 'f0', '2010', 'иметь', 'option', 'ощущение', 'женский', 'общение', 'хотеть', 'модем', 'отдел', 'площадка', 'супер', 'одноклассник', 'понравиться', 'повод', 'cheloveka', 'наличие', 'b8', 'рождение', 'язык', 'инженер', 'девчонка', 'симптом', 'film', 'cp', 'регион', 'рїрѕсђрѕрѕ', 'feed', 'вызывать', 'воспользоваться', 'политический', 'результат', 'пример', 'такси', 'мощность', 'служить', 'жильё', 'плеер', '15', 'довольно', 'время', 'тариф', 'sovmestimost', 'отказаться', 'щёлкнуть', 'лежалый', 'собака', 'оргазм', 'плохой', 'клип', 'зависеть', 'сознание', 'pl', 'производство', 'вывод', 'vkontakte', 'собрание', 'стиль', 'home', 'избранный', '25d0', 'шаг', 'киров', 'сильно', 'аналитик', '30', 'десятка', 'бренд', 'contacts', 'мода', 'оба', 'htm', 'интернет', 'аккумулятор', 'ip', 'качество', 'килограмм', 'лето', 'принтер', 'вести', 'arhiv', 'характер', 'baby', 'случай', 'www', '44', 'doc', 'бог', 'парень', 'выглядеть', 'позволять', 'бояться', 'тула', 'молодая', 'берег', 'здание', 'me', 'sitemap', 'nvidia', 'клиника', 'эффективный', 'поисковый', 'качать', 'igra', 'porno', '03', 'ибо', 'слой', 'производить'}\n"
     ]
    }
   ],
   "source": [
    "N=3000\n",
    "voc=set()\n",
    "idx = np.ravel(X_train.sum(axis=0).argsort(axis=1))[::-1][:N]\n",
    "top_words = np.array(vectorizer.get_feature_names())[idx].tolist()\n",
    "for word in top_words:\n",
    "    voc.add(word)\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=[]\n",
    "for doc in train_docs:\n",
    "    f1.append(np.array([np.float64(doc[3][0]),np.float64(doc[3][1]),np.float64(doc[3][3]),np.float64(doc[3][4]),np.float64(doc[3][5]),np.float64(doc[3][6])]))\n",
    "f1=np.array(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7044, 142036)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.27600000e+03, 2.18764052e-01, 3.19420035e-01, 1.10000000e+01,\n",
       "       3.57142857e-02, 2.60000000e+01])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vs=sparse.csc_matrix(f1).tocsr()\n",
    "#x_cos_train=sparse.csc_matrix(coss).tocsr()\n",
    "X_train_=sparse.hstack([X_train, x_vs]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7044, 47516)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[]\n",
    "for doc in train_docs:\n",
    "    y_train.append(float(doc[1]))\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_train.npy', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, y1, y2=train_test_split(X_train_, y_train,  test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "BayesianGaussianMixture\n",
      "BernoulliNB\n",
      "CalibratedClassifierCV\n",
      "ClassifierChain\n",
      "ComplementNB\n",
      "DecisionTreeClassifier\n",
      "DummyClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GaussianMixture\n",
      "GaussianNB\n",
      "GaussianProcessClassifier\n",
      "GradientBoostingClassifier\n",
      "GridSearchCV\n",
      "HistGradientBoostingClassifier\n",
      "KNeighborsClassifier\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "LinearDiscriminantAnalysis\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MLPClassifier\n",
      "MultiOutputClassifier\n",
      "MultinomialNB\n",
      "NuSVC\n",
      "OneVsRestClassifier\n",
      "Pipeline\n",
      "QuadraticDiscriminantAnalysis\n",
      "RFE\n",
      "RFECV\n",
      "RandomForestClassifier\n",
      "RandomizedSearchCV\n",
      "SGDClassifier\n",
      "SVC\n",
      "VotingClassifier\n",
      "_BinaryGaussianProcessClassifierLaplace\n",
      "_ConstantPredictor\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n",
    "clf2 = MultinomialNB()\n",
    "clf3 = SGDClassifier(alpha=0.0001, average=False,\n",
    "                     class_weight=None,\n",
    "                     early_stopping=False, epsilon=0.1,\n",
    "                     eta0=0.0, fit_intercept=True,\n",
    "                     l1_ratio=0.15,\n",
    "                     learning_rate='optimal',\n",
    "                     max_iter=1000,loss='log',\n",
    "                     n_iter_no_change=5, n_jobs=None,\n",
    "                     penalty='l2', power_t=0.5,\n",
    "                     random_state=None, shuffle=True,\n",
    "                     tol=0.001, validation_fraction=0.1)\n",
    "clf4= xgboost.XGBClassifier(max_depth=6,learning_rate=0.3,n_estimators=1000,scale_pos_weight=1.5, n_jobs=6,probability=True)\n",
    "clf6=SVC(C=1.0, cache_size=200, class_weight=None,\n",
    "                                  coef0=0.0, decision_function_shape='ovr',\n",
    "                                  degree=3, gamma='auto', kernel='rbf',\n",
    "                                  max_iter=-1,\n",
    "                                  random_state=None, shrinking=True, tol=0.001,\n",
    "                                  verbose=False, probability=True)\n",
    "clf7=RandomForestClassifier(n_jobs=6,n_estimators=1000,max_depth=6,class_weight='balanced')\n",
    "clf8=KNeighborsClassifier(n_neighbors=12)\n",
    "clf9=DecisionTreeClassifier(class_weight='balanced')\n",
    "neuronov_v_sloe=[]\n",
    "for j in range(10):\n",
    "    neuronov_v_sloe.append(100)\n",
    "clf10=MLPClassifier(hidden_layer_sizes=neuronov_v_sloe)\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('NB', clf2), ('sgd', clf3),\n",
    "                       ('xgb', clf4), ('svc', clf6), ('rf', clf7),\n",
    "            ('knn', clf8), ('dt', clf9), ('perceptron', clf10)], voting='soft',weights=[1, 1, 1, 2, 1.3, 2, 1,1.3,1.5], n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:50745\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>6</li>\n",
       "  <li><b>Memory: </b>17.11 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:50745' processes=3 cores=6>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend(\"dask\"):\n",
    "    eclf1.fit(X_train_, y_train)\n",
    "\n",
    "y_pred=eclf1.predict(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7044, 142036)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('train.npz', X_train_)\n",
    "scipy.sparse.save_npz('test.npz', X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8967193195625759 3 1 50\n",
      "0.8952380952380953 3 1.3 50\n",
      "0.891764705882353 3 1.5 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-92c61f8f37c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn_est\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_ests\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_est\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscale_pos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_est\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1110\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "depths=[3,4,5,6]\n",
    "spws=[1,1.3,1.5]\n",
    "n_ests=[50]#,300,500,700]\n",
    "for depth in depths:\n",
    "    for spw in spws:\n",
    "        for n_est in n_ests:\n",
    "            model = xgboost.XGBClassifier(max_depth=depth,learning_rate=0.3,n_estimators=n_est,scale_pos_weight=spw, n_jobs=6)\n",
    "            model.fit(x1,y1)\n",
    "            print(f1_score(y2, model.predict(x2)),depth,spw, n_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1  0.9869031377899045\n",
      "x2  0.9612015018773468\n",
      "количество слоев  12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-9cceb54ef111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mneuronov_v_sloe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneuronov_v_sloe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprediction_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprediction_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    980\u001b[0m         \"\"\"\n\u001b[0;32m    981\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m--> 982\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    356\u001b[0m         activations.extend(np.empty((batch_size, n_fan_out))\n\u001b[0;32m    357\u001b[0m                            for n_fan_out in layer_units[1:])\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mdeltas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_layer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    356\u001b[0m         activations.extend(np.empty((batch_size, n_fan_out))\n\u001b[0;32m    357\u001b[0m                            for n_fan_out in layer_units[1:])\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mdeltas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_layer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "neuronov_v_sloe=[]\n",
    "neuronov=[10,100,300,500,1000]\n",
    "for i in neuronov:\n",
    "    for j in range(i):\n",
    "        neuronov_v_sloe.append(100)\n",
    "    model=MLPClassifier(hidden_layer_sizes=neuronov_v_sloe).fit(x1,y1)\n",
    "    prediction_1=model.predict(x1)\n",
    "    prediction_2=model.predict(x2)\n",
    "    print(\"x1 \", f1_score(prediction_1,y1))\n",
    "    print(\"x2 \", f1_score(prediction_2,y2))\n",
    "    print(\"количество слоев \", model.n_layers_)\n",
    "    neuronov_v_sloe.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:04:30 INFO:Complete items 00000\n",
      "01:05:58 INFO:Complete items 00100\n",
      "01:07:13 INFO:Complete items 00200\n",
      "01:08:07 INFO:Complete items 00300\n",
      "01:09:53 INFO:Complete items 00400\n",
      "01:10:50 INFO:Complete items 00500\n",
      "01:12:00 INFO:Complete items 00600\n",
      "01:12:50 INFO:Complete items 00700\n",
      "01:13:53 INFO:Complete items 00800\n",
      "01:15:54 INFO:Complete items 00900\n",
      "01:17:49 INFO:Complete items 01000\n",
      "01:20:18 INFO:Complete items 01100\n",
      "01:21:21 INFO:Complete items 01200\n",
      "01:22:09 INFO:Complete items 01300\n",
      "01:23:15 INFO:Complete items 01400\n",
      "01:23:59 INFO:Complete items 01500\n",
      "01:24:50 INFO:Complete items 01600\n",
      "01:25:43 INFO:Complete items 01700\n",
      "01:26:25 INFO:Complete items 01800\n",
      "01:27:17 INFO:Complete items 01900\n",
      "01:28:30 INFO:Complete items 02000\n",
      "01:30:01 INFO:Complete items 02100\n",
      "01:31:30 INFO:Complete items 02200\n",
      "01:32:56 INFO:Complete items 02300\n",
      "01:33:44 INFO:Complete items 02400\n",
      "01:34:50 INFO:Complete items 02500\n",
      "01:35:52 INFO:Complete items 02600\n",
      "01:36:57 INFO:Complete items 02700\n",
      "01:37:36 INFO:Complete items 02800\n",
      "01:38:19 INFO:Complete items 02900\n",
      "01:39:19 INFO:Complete items 03000\n",
      "01:40:08 INFO:Complete items 03100\n",
      "01:41:03 INFO:Complete items 03200\n",
      "01:42:41 INFO:Complete items 03300\n",
      "01:43:40 INFO:Complete items 03400\n",
      "01:44:23 INFO:Complete items 03500\n",
      "01:45:19 INFO:Complete items 03600\n",
      "01:46:32 INFO:Complete items 03700\n",
      "01:47:16 INFO:Complete items 03800\n",
      "01:47:59 INFO:Complete items 03900\n",
      "01:48:55 INFO:Complete items 04000\n",
      "01:50:04 INFO:Complete items 04100\n",
      "01:51:13 INFO:Complete items 04200\n",
      "01:51:51 INFO:Complete items 04300\n",
      "01:52:47 INFO:Complete items 04400\n",
      "01:53:30 INFO:Complete items 04500\n",
      "01:54:08 INFO:Complete items 04600\n",
      "01:55:00 INFO:Complete items 04700\n",
      "01:55:50 INFO:Complete items 04800\n",
      "01:56:35 INFO:Complete items 04900\n",
      "01:57:21 INFO:Complete items 05000\n",
      "01:58:07 INFO:Complete items 05100\n",
      "01:59:05 INFO:Complete items 05200\n",
      "02:00:13 INFO:Complete items 05300\n",
      "02:01:19 INFO:Complete items 05400\n",
      "02:03:19 INFO:Complete items 05500\n",
      "02:03:58 INFO:Complete items 05600\n",
      "02:05:05 INFO:Complete items 05700\n",
      "02:05:51 INFO:Complete items 05800\n",
      "02:06:48 INFO:Complete items 05900\n",
      "02:07:52 INFO:Complete items 06000\n",
      "02:08:54 INFO:Complete items 06100\n",
      "02:09:44 INFO:Complete items 06200\n",
      "02:10:32 INFO:Complete items 06300\n",
      "02:11:39 INFO:Complete items 06400\n",
      "02:12:42 INFO:Complete items 06500\n",
      "02:13:32 INFO:Complete items 06600\n",
      "02:14:28 INFO:Complete items 06700\n",
      "02:15:12 INFO:Complete items 06800\n",
      "02:15:53 INFO:Complete items 06900\n",
      "02:16:47 INFO:Complete items 07000\n",
      "02:17:39 INFO:Complete items 07100\n",
      "02:18:31 INFO:Complete items 07200\n",
      "02:19:39 INFO:Complete items 07300\n",
      "02:20:51 INFO:Complete items 07400\n",
      "02:23:07 INFO:Complete items 07500\n",
      "02:24:06 INFO:Complete items 07600\n",
      "02:24:44 INFO:Complete items 07700\n",
      "02:26:05 INFO:Complete items 07800\n",
      "02:27:03 INFO:Complete items 07900\n",
      "02:27:43 INFO:Complete items 08000\n",
      "02:28:33 INFO:Complete items 08100\n",
      "02:29:11 INFO:Complete items 08200\n",
      "02:29:51 INFO:Complete items 08300\n",
      "02:30:51 INFO:Complete items 08400\n",
      "02:31:32 INFO:Complete items 08500\n",
      "02:32:30 INFO:Complete items 08600\n",
      "02:33:47 INFO:Complete items 08700\n",
      "02:34:58 INFO:Complete items 08800\n",
      "02:35:48 INFO:Complete items 08900\n",
      "02:36:33 INFO:Complete items 09000\n",
      "02:37:58 INFO:Complete items 09100\n",
      "02:39:24 INFO:Complete items 09200\n",
      "02:41:31 INFO:Complete items 09300\n",
      "02:42:40 INFO:Complete items 09400\n",
      "02:43:55 INFO:Complete items 09500\n",
      "02:44:56 INFO:Complete items 09600\n",
      "02:45:56 INFO:Complete items 09700\n",
      "02:46:36 INFO:Complete items 09800\n",
      "02:48:35 INFO:Complete items 09900\n",
      "02:49:27 INFO:Complete items 10000\n",
      "02:50:08 INFO:Complete items 10100\n",
      "02:51:25 INFO:Complete items 10200\n",
      "02:52:21 INFO:Complete items 10300\n",
      "02:53:05 INFO:Complete items 10400\n",
      "02:54:04 INFO:Complete items 10500\n",
      "02:54:50 INFO:Complete items 10600\n",
      "02:55:46 INFO:Complete items 10700\n",
      "02:57:23 INFO:Complete items 10800\n",
      "02:57:57 INFO:Complete items 10900\n",
      "02:58:35 INFO:Complete items 11000\n",
      "02:59:33 INFO:Complete items 11100\n",
      "03:00:17 INFO:Complete items 11200\n",
      "03:01:35 INFO:Complete items 11300\n",
      "03:02:40 INFO:Complete items 11400\n",
      "03:03:20 INFO:Complete items 11500\n",
      "03:04:05 INFO:Complete items 11600\n",
      "03:05:12 INFO:Complete items 11700\n",
      "03:06:18 INFO:Complete items 11800\n",
      "03:07:27 INFO:Complete items 11900\n",
      "03:08:07 INFO:Complete items 12000\n",
      "03:08:51 INFO:Complete items 12100\n",
      "03:09:36 INFO:Complete items 12200\n",
      "03:10:31 INFO:Complete items 12300\n",
      "03:11:10 INFO:Complete items 12400\n",
      "03:11:49 INFO:Complete items 12500\n",
      "03:12:29 INFO:Complete items 12600\n",
      "03:13:08 INFO:Complete items 12700\n",
      "03:13:56 INFO:Complete items 12800\n",
      "03:14:46 INFO:Complete items 12900\n",
      "03:15:51 INFO:Complete items 13000\n",
      "03:16:32 INFO:Complete items 13100\n",
      "03:18:21 INFO:Complete items 13200\n",
      "03:19:32 INFO:Complete items 13300\n",
      "03:20:22 INFO:Complete items 13400\n",
      "03:21:10 INFO:Complete items 13500\n",
      "03:21:54 INFO:Complete items 13600\n",
      "03:22:39 INFO:Complete items 13700\n",
      "03:23:36 INFO:Complete items 13800\n",
      "03:25:06 INFO:Complete items 13900\n",
      "03:26:05 INFO:Complete items 14000\n",
      "03:27:14 INFO:Complete items 14100\n",
      "03:28:25 INFO:Complete items 14200\n",
      "03:29:34 INFO:Complete items 14300\n",
      "03:31:19 INFO:Complete items 14400\n",
      "03:32:04 INFO:Complete items 14500\n",
      "03:32:47 INFO:Complete items 14600\n",
      "03:33:47 INFO:Complete items 14700\n",
      "03:34:46 INFO:Complete items 14800\n",
      "03:35:30 INFO:Complete items 14900\n",
      "03:36:17 INFO:Complete items 15000\n",
      "03:36:58 INFO:Complete items 15100\n",
      "03:37:46 INFO:Complete items 15200\n",
      "03:39:13 INFO:Complete items 15300\n",
      "03:40:08 INFO:Complete items 15400\n",
      "03:40:58 INFO:Complete items 15500\n",
      "03:42:40 INFO:Complete items 15600\n",
      "03:44:07 INFO:Complete items 15700\n",
      "03:44:55 INFO:Complete items 15800\n",
      "03:45:45 INFO:Complete items 15900\n",
      "03:46:30 INFO:Complete items 16000\n",
      "03:46:54 INFO:Complete items 16038\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_FILE  = 'kaggle_test_data_tab.csv.gz'\n",
    "test_docs = list(load_csv(TEST_DATA_FILE, calc_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23083"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_docs+test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=vectorizer.transform(generator(train_docss=test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "y_test=[]\n",
    "for doc in test_docs:\n",
    "    y_test.append(doc[1])\n",
    "y_test=np.array(y_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7044, 142036) (16039, 142036)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_.shape,X_test_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2=[]\n",
    "for doc in test_docs:\n",
    "    f2.append(np.array([np.float64(doc[3][0]),np.float64(doc[3][1]),np.float64(doc[3][3]),np.float64(doc[3][4]),np.float64(doc[3][5]),np.float64(doc[3][6])]))\n",
    "f2=np.array(f2)\n",
    "x_vs=sparse.csc_matrix(f2).tocsr()\n",
    "#x_cos_test=sparse.csc_matrix(coss_test).tocsr()\n",
    "X_test_=sparse.hstack([X_test, x_vs]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier(max_depth=6,learning_rate=0.3,n_estimators=1000,scale_pos_weight=1.5, n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=sparse.vstack([X_train_, X_test_]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelSpreading,LabelPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronov_v_sloe=[]\n",
    "for j in range(10):\n",
    "        neuronov_v_sloe.append(100)\n",
    "model=MLPClassifier(hidden_layer_sizes=neuronov_v_sloe).fit(X_train_,y_train)\n",
    "y_test=model.predict(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976456009913259"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_,y_train)\n",
    "y_pred=model.predict(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6835474206919194"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  \"\"\"\n",
      "Exception ignored in: <bound method Booster.__del__ of <xgboost.core.Booster object at 0x000002EDE18DAAC8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 956, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'Booster' object has no attribute 'handle'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "is_spam_data = [doc.features[0] for doc in train_docs if doc[1] == True]\n",
    "not_spam_data = [doc.features[0] for doc in train_docs if doc[1] == False]\n",
    "bins = range(0,3000,10)\n",
    "plt.hist(is_spam_data, bins=bins, color='red', normed=True, alpha=0.7, label='spam')\n",
    "plt.hist(not_spam_data, bins=bins, color='blue', normed=True, alpha=0.7, label='not_spam')\n",
    "plt.title('NumWords')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_spam_data = [doc.features[1] for doc in train_docs if doc[1] == True]\n",
    "not_spam_data = [doc.features[1] for doc in train_docs if doc[1] == False]\n",
    "bins = 100\n",
    "plt.hist(is_spam_data, bins=bins, color='red', normed=True, alpha=0.7, label='spam')\n",
    "plt.hist(not_spam_data, bins=bins, color='blue', normed=True, alpha=0.7, label='not_spam')\n",
    "plt.title('AvgWordLen')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_spam_data = [doc.features[2] for doc in train_docs if doc[1] == True]\n",
    "not_spam_data = [doc.features[2] for doc in train_docs if doc[1] == False]\n",
    "bins = range(0, 30, 1)\n",
    "plt.hist(is_spam_data, bins=bins, color='red', normed=True, alpha=0.7, label='spam')\n",
    "plt.hist(not_spam_data, bins=bins, color='blue', normed=True, alpha=0.7, label='not_spam')\n",
    "plt.title('NumWordsTitle')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Добавить графики для остальных фичей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Классификатор: **\n",
    "Нужно реализовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, doc):        \n",
    "        return doc.features[0]                     \n",
    "    \n",
    "    def predict_all(self, docs):\n",
    "        res = []\n",
    "        for doc_num, doc in enumerate(docs):\n",
    "            trace(doc_num)\n",
    "            prediction = self.predict(doc)            \n",
    "            res.append( (doc.doc_id, doc.is_spam, doc.url, prediction) )\n",
    "        return res\n",
    "    \n",
    "    def train(self, docs):                \n",
    "        for doc_num, doc in enumerate(docs):                            \n",
    "            #TODO вставить код обучения\n",
    "            trace(doc_num)\n",
    "        trace(doc_num, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = Classifier()\n",
    "classifier.train(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Посмотреть, что предсказывается для тестового html\n",
    "classifier.predict(DocItem(0, 0, test_url , test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Рисуем графики **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_results(train_docs, min_threshold=0, max_threshold=3000, step=100, trace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-53ecce949300>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'my_submission.csv'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Prediction'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "TEST_DATA_FILE  = 'kaggle/kaggle_test_data_tab.csv.gz'\n",
    "# TEST_DATA_FILE  = 'kaggle/kaggle_train_data_tab_300.csv.gz'\n",
    "\n",
    "#test_docs = load_csv(TEST_DATA_FILE, calc_features)\n",
    "\n",
    "#threshold = 800\n",
    "\n",
    "with open('my_submission.csv' , 'wb') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['Id','Prediction'])\n",
    "    for item,y in zip(classifier.predict_all(test_docs),y_test):\n",
    "        writer.writerow([item[0], y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "(150, 4) [ 0  0  0  0 -1 -1 -1  0  0  0 -1  0  0 -1 -1 -1  0  0  0 -1  0 -1 -1  0\n",
      "  0  0 -1  0  0 -1  0 -1 -1  0  0  0  0 -1  0  0 -1  0 -1  0 -1  0  0  0\n",
      "  0 -1  1  1  1  1  1  1 -1 -1 -1  1  1 -1  1  1 -1  1 -1  1 -1  1  1 -1\n",
      " -1  1  1  1  1 -1  1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1 -1 -1 -1  2  2  2  2 -1  2  2 -1 -1 -1 -1  2  2  2  2  2 -1  2  2\n",
      "  2  2  2 -1 -1  2  2  2 -1  2  2 -1 -1  2  2  2  2  2  2  2  2 -1  2  2\n",
      " -1 -1  2  2 -1 -1] [False False False False  True  True  True False False False  True False\n",
      " False  True  True  True False False False  True False  True  True False\n",
      " False False  True False False  True False  True  True False False False\n",
      " False  True False False  True False  True False  True False False False\n",
      " False  True False False False False False False  True  True  True False\n",
      " False  True False False  True False  True False  True False False  True\n",
      "  True False False False False  True False  True False False False  True\n",
      " False False False False False False  True False False False False False\n",
      " False False  True  True  True False False False False  True False False\n",
      "  True  True  True  True False False False False False  True False False\n",
      " False False False  True  True False False False  True False False  True\n",
      "  True False False False False False False False False  True False False\n",
      "  True  True False False  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"my_submission.csv\", \"wb\")\n",
    "file.write( bytes(str(\"Id,Prediction\\n\"), \"utf-8\"))\n",
    "for i in range(len(test_docs)):\n",
    "    file.write(bytes(str(test_docs[i][0]) + \",\" +str(int(y_pred[i])) + str(\"\\n\"), \"utf-8\"))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-9818452cb728>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#np.save('train_docs.npy', np.array(train_docs, implace=True))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_docs.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m--> 536\u001b[1;33m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    537\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[1;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#np.save('train_docs.npy', np.array(train_docs, implace=True))\n",
    "np.save('test_docs.npy', test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('test_docs', \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    file.cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_docs, open('train.p', 'wb'))\n",
    "pickle.dump(test_docs, open('test.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = open('train.p', 'rb')\n",
    "input_test = open('test.p', 'rb')\n",
    "train_docs=pickle.load(input_train)\n",
    "test_docs=pickle.load(input_test)\n",
    "input_test.close()\n",
    "input_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(doc), tags=[i]) for i,doc in enumerate(generator(train_docss=train_docs+test_docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [ -1.02624     -9.412314   -15.912084     6.599049     4.1291685\n",
      "  -9.896153     2.264849    -5.840065    -7.396797     3.791914\n",
      "  -4.566124     2.3207166   -4.907215     1.9643959    3.4458458\n",
      "   2.1727035    0.03726032  11.27989    -10.212159    -4.746123\n",
      "  14.658656    -0.17648442   2.917683     2.5123615    6.1782126\n",
      "  -8.106794    -2.5798273    6.32896     -5.7203345   -0.546161\n",
      "  11.339916   -10.809691    11.899398    -2.969841     3.7725005\n",
      "  -1.1467489    5.6229506   -6.3758616    5.0033298   -2.5071573\n",
      "   6.8345933    1.2167974   -1.9114177    1.093597    -8.572602\n",
      "  -6.1242285    9.578417    -6.235271    -3.9235585   -0.24970564\n",
      "   6.349547     1.6324854  -10.67436      4.009813     9.015545\n",
      "   3.646399    -5.690334   -11.976712    -1.5372528   -2.7370815\n",
      "   5.014127    -2.9239216    3.8877947    1.283477    -5.373032\n",
      "   2.551737    13.521847    -6.4487524    7.5171375   -1.6247194\n",
      "   6.7707753    6.1042285    2.2689226   -7.456477    10.813289\n",
      "   7.2756944    8.5267725   -5.21694     -2.2832174    5.0626216\n",
      "  -9.328549    -2.6692061   -6.057347     0.90125763   6.8365617\n",
      "  -3.7327573   -5.5050917   -5.207127     2.7011442    3.6340911\n",
      "  -8.086008    -7.590353    -0.4143426    4.9547453   -6.2254\n",
      "  -5.1363297   -3.2667994    3.8520355   -1.734177    -5.148848  ]\n"
     ]
    }
   ],
   "source": [
    "test_data=test_docs[10][3][2]\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7043..."
     ]
    }
   ],
   "source": [
    "coss=[]\n",
    "cos=[]\n",
    "for i in range(len(train_docs)):\n",
    "    cos.clear()\n",
    "    for j in range(len(train_docs)):\n",
    "        similar_doc = model.docvecs.distance(i,j)\n",
    "        cos.append(similar_doc)\n",
    "    coss.append(cos)\n",
    "    sys.stdout.write('\\r {0}...'.format(i))\n",
    "coss=np.matrix(coss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23082..."
     ]
    }
   ],
   "source": [
    "coss_test=[]\n",
    "cos_test=[]\n",
    "for i in range(len(train_docs),len(test_docs+train_docs)):\n",
    "    cos_test.clear()\n",
    "    for j in range(len(train_docs)):\n",
    "        similar_doc = model.docvecs.distance(i,j)\n",
    "        cos_test.append(similar_doc)\n",
    "    coss_test.append(cos)\n",
    "    sys.stdout.write('\\r {0}...'.format(i))\n",
    "coss_test=np.matrix(coss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16039, 7044)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coss_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16039"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('coss_train.npy', coss)\n",
    "np.save('coss_test.npy', coss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
